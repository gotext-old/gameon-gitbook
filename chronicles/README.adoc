= Appendix: The Chronicles
:icons: font
:toc: preamble
:toc-title: 
:toclevels: 1
:imagesdir: /images

:signedHeaders: link:../security/signed_headers.adoc
:websocketProtocol: link:../microservices/WebsocketProtocol.adoc

The following sections are more like a blog than a book. Creating a new application from scratch using microservices is a messy business: documenting the process is, in some respects, as important as showing the end result.

Given this is an opensource book, we invite you to contribute your own experiences to this section.

== The first pass

image:Progression.001.jpeg[Progression, width=600]

Our first attempt was typical stand-things-up-in-a hurry hacking. This was a bit of a hybrid: we did start this as separate runtime services, but they were built in more monolithic chunks. 

At this early early stage, it was a bunch of us (Java developers) standing these services up on our local machines. We were able to share code (thank you git), but relied on gradle and and local WDT dev/test environments to make progress without having these services deployed out into cloud-space.

The API between Concierge and Room was changing all the time in the early days, so having those two services built in the same repository made things easier as we kept changing our minds about how we wanted these two services to interact.

The Player service in this pass is also multifunction: it is serving the web front end, acting as the authenticator, and acting as the mediator between the player's device and the target room.


== Automation and Cloud

image:Progression.002.jpeg[Progression, width=600]

Automated build pipelines arrived! and Docker containers out in the cloud! 
Thankfully for those of us doing most of the dev work, the creation, testing of the delivery pipeline was pretty transparent. The build pipeline is triggered by a commit to the master branches of our repositories, which means no change to our usual workflow.

A proxy was added to help route requests between the services and docker containers, and to give us a seamless domain space (game-on.org).

== Splitting out the web front-end

image:Progression.003.jpeg[Progression, width=600]

There is no good reason (and many not-good reasons) to have your web front end packed in with a Java application. Freeing your front-end developers to be able to develop and test without worrying about the Java gorp is a good thing. 

We therefore split the web stuff into its own (small, simple) container. This had the added 'bonus' of requiring CORS headers for local development, which was a pain. In production, the proxy alleviated these issues: all of the services belonged to the same domain. 

This could be countered using xip.io, but for that to work, you still need to be connected to the internet to allow the xio.io domain to resolve.

We added CORS filters to the app for the time being and moved on.

== Something other than Java!

image:Progression.004.jpeg[Progression, width=600]

Our team has nothing against JavaScript, but Java is what we know. When attempting something big and new, we felt it usually good idea to constrain the number of moving parts in the air at once. In this case, we stuck with a language we were familiar with for the core services. 

The Node.js room brought JS to the backend. More will follow.

== Explain it with pinache

image:Progression.005.jpeg[Progression, width=600]

Swagger documentation!

Player and Concierge both have a REST API, and the Swagger container hosts and serves the document that presents both APIs together in one view.

The bulk of the traffic flowing around in the game is over WebSockets, and that is difficult for Swagger to document. For those interested in the real low-level details, we wrote down and iterated on the WebSocket protocol in a Box note. The result is documented {websocketProtocol}[here].

== Splitting up the Player service

image:Progression.006.jpeg[Progression, width=600]

Over time, the Player service, which was filling multiple roles, felt more and more out of place. It was already cut down once, when we moved the webapp out, but the mediator function was very complex, and will have different load/scaling requirements at the end of the day. 

* The Player service (even if we keep the auth and player datastore together) is REST-only, which has fairly well understood load/scaling characteristics.  
* The mediator function, on the other hand, is managing approximately two websocket connections per connected user. Data will be flowing through the mediator in a much different way. 

Lots of reasons. 

We did finally split the two functions apart, which was not difficult. They were indepdendent of each other, but happened to be built/packaged together in the same app from within the same project. It was almost like we knew we would have to do this when we started...

==  Bringing in etcd

image:Progression.007.jpeg[Progression, width=600]

So, at this point, we had a good fistful of services going through deployment pipelines, and we were fresh from the effort of adding a new one (for mediator). Managing environment variables across several build pipelines is a pain: lots of clicking lots of boxes to get to lots of little perfect UI views. 

We moved runtime environment variables particular to our production docker environment out into etcd. This simplified what we needed to configure in our build pipelines, and made it easier for containers to pick up new values when they were restarted (no dynamic reconfiguration yet).

== Buh-bye to Concierge!

image:Progression.008.jpeg[Progression, width=600]

Concierge was such a cute service! It was supposed to help players find their way from room to room. But it's name was confusing (was it a map or a traffic director?). It's interaction pattern with mediator also introduced a single point of failure: if the Concierge was down, the Players were pretty much stuck wherever they were, with a fallback to First Room. 

We designed a new Map service, with a much better API for adding and removing rooms from the map. We built the new map service in place, alongside the Concierge and other core services. Once the Map was up and stable, we started moving our other services over to using Map instead, with Concierge still humming happily along until its last consumer was cleaned up.

== Map Auth

image:Progression.009.jpeg[Progression, width=600]

We added {signedHeaders}[signed header-based security] to the Map service APIs.

== More Rooms!

image:Progression.010.jpeg[Progression, width=600]

Rooms written in Go, another room in JavaScript, and two more in Java. We learned lots as other people were able to try adding their own rooms, especially as some of them were trying to build their rooms while the Map/Concierge switch was in flight. Ouch.

____
In the end, I realize that my experience was different from what a new developer today would experience. We were building on shifting sand, they will be building on a more stable API set. Too bad for them; building on sand was its own kind of adventure game.
____

== Enter: The Sweep

image:Progression.011.jpeg[Progression, width=600]

Our first non-player character! The Sweep is in early days, so we don't know in what directions it will grow, but it will serve an important function: keeping the map alive and vital by pruning sick rooms.
